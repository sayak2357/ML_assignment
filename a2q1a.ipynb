{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqIWUS4yASP1"
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00HbLljmTaWd"
   },
   "source": [
    "# loading the binary class Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2176
    },
    "colab_type": "code",
    "id": "HEGzC6FSIma0",
    "outputId": "1eb19df3-f4df-446a-dad2-862e15959208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  \\\n",
      "0               842     0          2.2         0   1       0           7   \n",
      "1              1021     1          0.5         1   0       1          53   \n",
      "2               563     1          0.5         1   2       1          41   \n",
      "3               615     1          2.5         0   0       0          10   \n",
      "4              1821     1          1.2         0  13       1          44   \n",
      "5              1859     0          0.5         1   3       0          22   \n",
      "6              1821     0          1.7         0   4       1          10   \n",
      "7              1954     0          0.5         1   0       0          24   \n",
      "8              1445     1          0.5         0   0       0          53   \n",
      "9               509     1          0.6         1   2       1           9   \n",
      "10              769     1          2.9         1   0       0           9   \n",
      "11             1520     1          2.2         0   5       1          33   \n",
      "12             1815     0          2.8         0   2       0          33   \n",
      "13              803     1          2.1         0   7       0          17   \n",
      "14             1866     0          0.5         0  13       1          52   \n",
      "15              775     0          1.0         0   3       0          46   \n",
      "16              838     0          0.5         0   1       1          13   \n",
      "17              595     0          0.9         1   7       1          23   \n",
      "18             1131     1          0.5         1  11       0          49   \n",
      "19              682     1          0.5         0   4       0          19   \n",
      "20              772     0          1.1         1  12       0          39   \n",
      "21             1709     1          2.1         0   1       0          13   \n",
      "22             1949     0          2.6         1   4       0          47   \n",
      "23             1602     1          2.8         1   4       1          38   \n",
      "24              503     0          1.2         1   5       1           8   \n",
      "25              961     1          1.4         1   0       1          57   \n",
      "26              519     1          1.6         1   7       1          51   \n",
      "27              956     0          0.5         0   1       1          41   \n",
      "28             1453     0          1.6         1  12       1          52   \n",
      "29              851     0          0.5         0   3       0          21   \n",
      "...             ...   ...          ...       ...  ..     ...         ...   \n",
      "1970           1913     1          1.8         0   0       0          29   \n",
      "1971            538     0          1.1         1   0       1          25   \n",
      "1972           1191     0          0.8         0   6       1          46   \n",
      "1973            816     0          3.0         1   2       0           9   \n",
      "1974            915     1          0.5         1   9       1          33   \n",
      "1975           1157     1          0.8         0   7       0          27   \n",
      "1976           1201     1          0.5         0   2       0          10   \n",
      "1977           1379     0          1.1         1   1       1          18   \n",
      "1978           1483     1          2.2         0   3       1          53   \n",
      "1979           1614     0          1.2         0   1       1           9   \n",
      "1980            930     1          1.0         1   4       1           4   \n",
      "1981           1454     0          2.6         0   8       0           6   \n",
      "1982           1784     0          1.6         0   4       0          41   \n",
      "1983           1262     0          1.8         1  12       0          34   \n",
      "1984            797     0          2.2         1   0       0          37   \n",
      "1985           1829     1          2.1         0   8       0          59   \n",
      "1986           1139     1          0.9         1   6       1          58   \n",
      "1987            618     1          1.0         0   9       1          13   \n",
      "1988           1547     1          2.9         0   2       0          57   \n",
      "1989            586     0          2.8         0   2       0          15   \n",
      "1990           1617     1          2.4         0   8       1          36   \n",
      "1991           1882     0          2.0         0  11       1          44   \n",
      "1992            674     1          2.9         1   1       0          21   \n",
      "1993           1467     1          0.5         0   0       0          18   \n",
      "1994            858     0          2.2         0   1       0          50   \n",
      "1995            794     1          0.5         1   0       1           2   \n",
      "1996           1965     1          2.6         1   0       0          39   \n",
      "1997           1911     0          0.9         1   1       1          36   \n",
      "1998           1512     0          0.9         0   4       1          46   \n",
      "1999            510     1          2.0         1   5       1          45   \n",
      "\n",
      "      m_dep  mobile_wt  n_cores     ...       px_height  px_width   ram  sc_h  \\\n",
      "0       0.6        188        2     ...              20       756  2549     9   \n",
      "1       0.7        136        3     ...             905      1988  2631    17   \n",
      "2       0.9        145        5     ...            1263      1716  2603    11   \n",
      "3       0.8        131        6     ...            1216      1786  2769    16   \n",
      "4       0.6        141        2     ...            1208      1212  1411     8   \n",
      "5       0.7        164        1     ...            1004      1654  1067    17   \n",
      "6       0.8        139        8     ...             381      1018  3220    13   \n",
      "7       0.8        187        4     ...             512      1149   700    16   \n",
      "8       0.7        174        7     ...             386       836  1099    17   \n",
      "9       0.1         93        5     ...            1137      1224   513    19   \n",
      "10      0.1        182        5     ...             248       874  3946     5   \n",
      "11      0.5        177        8     ...             151      1005  3826    14   \n",
      "12      0.6        159        4     ...             607       748  1482    18   \n",
      "13      1.0        198        4     ...             344      1440  2680     7   \n",
      "14      0.7        185        1     ...             356       563   373    14   \n",
      "15      0.7        159        2     ...             862      1864   568    17   \n",
      "16      0.1        196        8     ...             984      1850  3554    10   \n",
      "17      0.1        121        3     ...             441       810  3752    10   \n",
      "18      0.6        101        5     ...             658       878  1835    19   \n",
      "19      1.0        121        4     ...             902      1064  2337    11   \n",
      "20      0.8         81        7     ...            1314      1854  2819    17   \n",
      "21      1.0        156        2     ...             974      1385  3283    17   \n",
      "22      0.3        199        4     ...             407       822  1433    11   \n",
      "23      0.7        114        3     ...             466       788  1037     8   \n",
      "24      0.4        111        3     ...             201      1245  2583    11   \n",
      "25      0.6        114        8     ...             291      1434  2782    18   \n",
      "26      0.3        132        4     ...             550       645  3763    16   \n",
      "27      1.0        143        7     ...             511      1075  3286    17   \n",
      "28      0.3         96        2     ...             187      1311  2373    10   \n",
      "29      0.4        200        5     ...            1171      1263   478    12   \n",
      "...     ...        ...      ...     ...             ...       ...   ...   ...   \n",
      "1970    0.6        111        5     ...             675       742  2023    17   \n",
      "1971    0.3        163        7     ...             455       537  2215     9   \n",
      "1972    0.8         89        6     ...              42       807   824    19   \n",
      "1973    0.1        117        1     ...            1196      1651  3851    10   \n",
      "1974    0.3        199        2     ...             503       986  2156     7   \n",
      "1975    0.1         88        8     ...            1694      1798  2885     8   \n",
      "1976    1.0         99        7     ...             306       558   495    15   \n",
      "1977    0.2        129        2     ...             838       885  2358    10   \n",
      "1978    0.7        169        5     ...             291       651  1744     6   \n",
      "1979    0.1        161        3     ...             173      1219  1832    15   \n",
      "1980    0.9        144        8     ...            1017      1289  2016    13   \n",
      "1981    0.4        199        3     ...             698      1018  1300    10   \n",
      "1982    0.4        164        6     ...             610      1437  2313    14   \n",
      "1983    0.1        149        5     ...             223       737  3248    13   \n",
      "1984    0.9        144        7     ...             206      1167  2216     9   \n",
      "1985    0.1         91        5     ...            1457      1919  3142    16   \n",
      "1986    0.5        161        2     ...             742       999  1850     9   \n",
      "1987    0.1         80        4     ...             591       724  1424    15   \n",
      "1988    0.4        114        1     ...             347       957  1620     9   \n",
      "1989    0.2         83        3     ...             241       854  2592    12   \n",
      "1990    0.8         85        1     ...             743      1426   296     5   \n",
      "1991    0.8        113        8     ...               4       743  3579    19   \n",
      "1992    0.2        198        3     ...             576      1809  1180     6   \n",
      "1993    0.6        122        5     ...             888      1099  3962    15   \n",
      "1994    0.1         84        1     ...             528      1416  3978    17   \n",
      "1995    0.8        106        6     ...            1222      1890   668    13   \n",
      "1996    0.2        187        4     ...             915      1965  2032    11   \n",
      "1997    0.7        108        8     ...             868      1632  3057     9   \n",
      "1998    0.1        145        5     ...             336       670   869    18   \n",
      "1999    0.9        168        6     ...             483       754  3919    19   \n",
      "\n",
      "      sc_w  talk_time  three_g  touch_screen  wifi  price_range  \n",
      "0        7         19        0             0     1            0  \n",
      "1        3          7        1             1     0            1  \n",
      "2        2          9        1             1     0            1  \n",
      "3        8         11        1             0     0            1  \n",
      "4        2         15        1             1     0            0  \n",
      "5        1         10        1             0     0            0  \n",
      "6        8         18        1             0     1            1  \n",
      "7        3          5        1             1     1            0  \n",
      "8        1         20        1             0     0            0  \n",
      "9       10         12        1             0     0            0  \n",
      "10       2          7        0             0     0            1  \n",
      "11       9         13        1             1     1            1  \n",
      "12       0          2        1             0     0            0  \n",
      "13       1          4        1             0     1            1  \n",
      "14       9          3        1             0     1            0  \n",
      "15      15         11        1             1     1            0  \n",
      "16       9         19        1             0     1            1  \n",
      "17       2         18        1             1     0            1  \n",
      "18      13         16        1             1     0            0  \n",
      "19       1         18        0             1     1            0  \n",
      "20      15          3        1             1     0            1  \n",
      "21       1         15        1             0     0            1  \n",
      "22       5         20        0             0     1            0  \n",
      "23       7         20        1             0     0            0  \n",
      "24       0         12        1             0     0            0  \n",
      "25       9          7        1             1     1            1  \n",
      "26       1          4        1             0     1            1  \n",
      "27       8         12        1             1     0            1  \n",
      "28       1         10        1             1     1            1  \n",
      "29       7         10        1             0     1            0  \n",
      "...    ...        ...      ...           ...   ...          ...  \n",
      "1970    13          8        1             1     0            1  \n",
      "1971     3         17        1             1     1            0  \n",
      "1972    18          7        1             0     0            0  \n",
      "1973     3         14        1             0     1            1  \n",
      "1974     3         13        1             1     0            0  \n",
      "1975     4          2        1             0     1            1  \n",
      "1976     6         14        1             1     1            0  \n",
      "1977     5         15        1             1     0            1  \n",
      "1978     3         10        1             0     0            0  \n",
      "1979     8         11        1             0     0            0  \n",
      "1980    10         16        1             1     1            0  \n",
      "1981     0          2        0             0     1            0  \n",
      "1982     1         11        0             1     0            1  \n",
      "1983     3          4        0             1     1            1  \n",
      "1984     5          6        1             0     0            0  \n",
      "1985     6          5        1             1     1            1  \n",
      "1986     4          8        1             0     0            0  \n",
      "1987    12          7        1             1     0            0  \n",
      "1988     2         19        0             1     1            0  \n",
      "1989     8          3        0             0     0            0  \n",
      "1990     3          7        1             0     0            0  \n",
      "1991     8         20        1             1     0            1  \n",
      "1992     3          4        1             1     1            0  \n",
      "1993    11          5        1             1     1            1  \n",
      "1994    16          3        1             1     0            1  \n",
      "1995     4         19        1             1     0            0  \n",
      "1996    10         16        1             1     1            1  \n",
      "1997     1          5        1             1     0            1  \n",
      "1998    10         19        1             1     1            0  \n",
      "1999     4          2        1             1     1            1  \n",
      "\n",
      "[2000 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "data = pd.read_csv('BinaryClass_Mobile_Price_train.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSvo4csEsbsr"
   },
   "source": [
    "#  random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EmysHWLeBHHa"
   },
   "outputs": [],
   "source": [
    "# Split a dataset into k folds and does a cross validation split\n",
    "def crossvalidator(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    " \n",
    "# Calculate accuracy percentage on the test and train data\n",
    "def accuracycalculator(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "scores=[]\n",
    "def algorithm(dataset, algorithm, n_folds, *args):\n",
    "  dataset_copy=list(dataset)\n",
    "  train_set=dataset_copy[:int(len(dataset_copy)*(7/10))] #Training data is first 70%\n",
    "  test_set=dataset_copy[int(len(dataset_copy)*(7/10)):] #Test data is last 30%\n",
    "  predicted = algorithm(train_set, test_set, *args)\n",
    "  actual = [row[-1] for row in test_set]\n",
    "  accuracy = accuracycalculator(actual, predicted)\n",
    "  scores.append(accuracy)\n",
    "  return scores\n",
    " \n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def datasetsplitter(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    " \n",
    "# Calculate the Gini index for a split dataset\n",
    "def giniindexcalculator(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini\n",
    " \n",
    "# Select the best split point for a dataset\n",
    "def bestsplit(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = datasetsplitter(index, row[index], dataset)\n",
    "            gini = giniindexcalculator(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    " \n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = bestsplit(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = bestsplit(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
    " \n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    root = bestsplit(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    " \n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    " \n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while(len(sample) < n_sample):\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n",
    " \n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)\n",
    " \n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = list()\n",
    "    for i in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "        trees.append(tree)\n",
    "        predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "HnM7Nh2_tZ9y",
    "outputId": "ff3db13c-5ed1-4fe2-adb4-3cad62525540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees:  1\n",
      "Scores:  [89.83333333333333]\n",
      "Mean Accuracy:  89.83333333333333\n",
      "Trees:  5\n",
      "Scores:  [89.83333333333333, 92.33333333333333]\n",
      "Mean Accuracy:  91.08333333333333\n",
      "Trees:  10\n",
      "Scores:  [89.83333333333333, 92.33333333333333, 93.0]\n",
      "Mean Accuracy:  91.72222222222221\n"
     ]
    }
   ],
   "source": [
    "dataset = data.values.tolist()\n",
    "# evaluate algorithm\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "n_folds = 5\n",
    "features = int(sqrt(len(dataset[0])-1))\n",
    "for trees in [1, 5, 10]:\n",
    "    scores = algorithm(dataset, random_forest, n_folds, max_depth, min_size, 1.0, trees, features)\n",
    "    print('Trees: '  , trees)\n",
    "    print('Scores: '  , scores)\n",
    "    print('Mean Accuracy: ' , (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2a.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
